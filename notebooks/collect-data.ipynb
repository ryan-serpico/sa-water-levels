{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect & clean data\n",
    "\n",
    "This notebook is dedicated to collecting, cleaning and analyzing all the data that we need for our story. This includes:\n",
    "- Historical water level data for the Edwards Aquifer.\n",
    "- Historical fullness data for several nearby lakes.\n",
    "- Historical discharge data for several nearby rivers.\n",
    "\n",
    "I will list out the sources of the data as we go along.\n",
    "\n",
    "* [Imports](#Imports)\n",
    "* [Edwards Aquifer water levels](#Edwards-Aquifer-water-levels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Let's start our project by importing all the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To automate the collection of certain datasets, we'll use beautifulsoup4 and requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# We use this library to streamline the process of downloading data from the USGS\n",
    "import dataretrieval.nwis as nwis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edwards Aquifer water levels\n",
    "\n",
    "The first dataset we'll collect and clean is the Edwards Aquifer Authority's J17 aquifer data [from their historical data page](https://www.edwardsaquifer.org/science-maps/aquifer-data/historical-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>DailyHighDate</th>\n",
       "      <th>WaterLevelElevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J17WL</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>635.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J17WL</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>635.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J17WL</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>636.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J17WL</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>636.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J17WL</td>\n",
       "      <td>2023-02-26</td>\n",
       "      <td>637.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Site DailyHighDate  WaterLevelElevation\n",
       "0  J17WL    2023-03-02               635.55\n",
       "1  J17WL    2023-03-01               635.98\n",
       "2  J17WL    2023-02-28               636.29\n",
       "3  J17WL    2023-02-27               636.80\n",
       "4  J17WL    2023-02-26               637.31"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edwards_aquifer_authority_url = 'https://www.edwardsaquifer.org/science-maps/aquifer-data/historical-data/'\n",
    "\n",
    "def getSoup(url):\n",
    "    page = requests.get(url) \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "soup = getSoup(edwards_aquifer_authority_url)\n",
    "\n",
    "# Find the href that contains the word 'csv'\n",
    "csv_links = soup.find_all('a', href=lambda href: href and 'csv' in href)\n",
    "j17_historical_data = csv_links[0]['href']\n",
    "\n",
    "# Import j17_historical_data into a pandas dataframe\n",
    "j17_historical_data_df = pd.read_csv(j17_historical_data)\n",
    "\n",
    "j17_historical_data_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we successfully used beautiful soup to grab the J17 data off of the historical data page and imported it into a pandas dataframe. Now we'll clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Year</th>\n",
       "      <th>dw_date</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>...</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2050</td>\n",
       "      <td>663.46</td>\n",
       "      <td>676.72</td>\n",
       "      <td>684.80</td>\n",
       "      <td>694.75</td>\n",
       "      <td>679.44</td>\n",
       "      <td>697.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>666.10</td>\n",
       "      <td>689.19</td>\n",
       "      <td>...</td>\n",
       "      <td>640.07</td>\n",
       "      <td>633.79</td>\n",
       "      <td>666.70</td>\n",
       "      <td>685.35</td>\n",
       "      <td>666.10</td>\n",
       "      <td>686.42</td>\n",
       "      <td>671.94</td>\n",
       "      <td>663.78</td>\n",
       "      <td>663.84</td>\n",
       "      <td>637.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/02/2050</td>\n",
       "      <td>663.90</td>\n",
       "      <td>676.59</td>\n",
       "      <td>684.68</td>\n",
       "      <td>694.64</td>\n",
       "      <td>679.44</td>\n",
       "      <td>697.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>666.00</td>\n",
       "      <td>689.10</td>\n",
       "      <td>...</td>\n",
       "      <td>640.69</td>\n",
       "      <td>634.04</td>\n",
       "      <td>666.37</td>\n",
       "      <td>685.41</td>\n",
       "      <td>665.94</td>\n",
       "      <td>686.20</td>\n",
       "      <td>672.13</td>\n",
       "      <td>663.88</td>\n",
       "      <td>663.63</td>\n",
       "      <td>637.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/03/2050</td>\n",
       "      <td>663.92</td>\n",
       "      <td>676.38</td>\n",
       "      <td>684.40</td>\n",
       "      <td>694.41</td>\n",
       "      <td>679.42</td>\n",
       "      <td>697.02</td>\n",
       "      <td>677.49</td>\n",
       "      <td>665.84</td>\n",
       "      <td>688.98</td>\n",
       "      <td>...</td>\n",
       "      <td>640.87</td>\n",
       "      <td>634.41</td>\n",
       "      <td>666.69</td>\n",
       "      <td>685.51</td>\n",
       "      <td>665.62</td>\n",
       "      <td>686.32</td>\n",
       "      <td>671.81</td>\n",
       "      <td>664.03</td>\n",
       "      <td>663.25</td>\n",
       "      <td>637.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/04/2050</td>\n",
       "      <td>663.56</td>\n",
       "      <td>676.40</td>\n",
       "      <td>684.22</td>\n",
       "      <td>694.35</td>\n",
       "      <td>679.27</td>\n",
       "      <td>697.03</td>\n",
       "      <td>677.13</td>\n",
       "      <td>666.96</td>\n",
       "      <td>688.88</td>\n",
       "      <td>...</td>\n",
       "      <td>640.99</td>\n",
       "      <td>634.73</td>\n",
       "      <td>667.70</td>\n",
       "      <td>685.12</td>\n",
       "      <td>665.09</td>\n",
       "      <td>686.50</td>\n",
       "      <td>671.69</td>\n",
       "      <td>663.86</td>\n",
       "      <td>663.02</td>\n",
       "      <td>636.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/05/2050</td>\n",
       "      <td>663.44</td>\n",
       "      <td>676.45</td>\n",
       "      <td>684.51</td>\n",
       "      <td>694.43</td>\n",
       "      <td>678.96</td>\n",
       "      <td>697.04</td>\n",
       "      <td>677.09</td>\n",
       "      <td>667.88</td>\n",
       "      <td>688.99</td>\n",
       "      <td>...</td>\n",
       "      <td>641.71</td>\n",
       "      <td>634.93</td>\n",
       "      <td>667.55</td>\n",
       "      <td>684.81</td>\n",
       "      <td>664.44</td>\n",
       "      <td>686.72</td>\n",
       "      <td>671.66</td>\n",
       "      <td>663.82</td>\n",
       "      <td>662.89</td>\n",
       "      <td>636.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Year     dw_date    2000    2001    2002    2003    2004    2005    2006  \\\n",
       "0     01/01/2050  663.46  676.72  684.80  694.75  679.44  697.33     NaN   \n",
       "1     01/02/2050  663.90  676.59  684.68  694.64  679.44  697.29     NaN   \n",
       "2     01/03/2050  663.92  676.38  684.40  694.41  679.42  697.02  677.49   \n",
       "3     01/04/2050  663.56  676.40  684.22  694.35  679.27  697.03  677.13   \n",
       "4     01/05/2050  663.44  676.45  684.51  694.43  678.96  697.04  677.09   \n",
       "\n",
       "Year    2007    2008  ...    2014    2015    2016    2017    2018    2019  \\\n",
       "0     666.10  689.19  ...  640.07  633.79  666.70  685.35  666.10  686.42   \n",
       "1     666.00  689.10  ...  640.69  634.04  666.37  685.41  665.94  686.20   \n",
       "2     665.84  688.98  ...  640.87  634.41  666.69  685.51  665.62  686.32   \n",
       "3     666.96  688.88  ...  640.99  634.73  667.70  685.12  665.09  686.50   \n",
       "4     667.88  688.99  ...  641.71  634.93  667.55  684.81  664.44  686.72   \n",
       "\n",
       "Year    2020    2021    2022    2023  \n",
       "0     671.94  663.78  663.84  637.61  \n",
       "1     672.13  663.88  663.63  637.40  \n",
       "2     671.81  664.03  663.25  637.16  \n",
       "3     671.69  663.86  663.02  636.72  \n",
       "4     671.66  663.82  662.89  636.41  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's convert the \"DailyHighDate\" column to a datetime object\n",
    "j17_historical_data_df['DailyHighDate'] = pd.to_datetime(j17_historical_data_df['DailyHighDate'])\n",
    "\n",
    "# Let's create a new column titled \"Year\" that contains the year of the \"DailyHighDate\" column\n",
    "j17_historical_data_df['Year'] = j17_historical_data_df['DailyHighDate'].dt.year\n",
    "\n",
    "# Let's create a new column titled \"dw_date\" that duplicates the data found in the \"DailyHighDate\" column, but replaces the year with 2050. We're doing all of this to make it easier for us to visualize it in a Datawrapper line chart in the future. The 2050 year will not appear in the final product and has no bearing on our analysis.\n",
    "j17_historical_data_df['dw_date'] = j17_historical_data_df['DailyHighDate'].dt.strftime('%m/%d') + '/2050'\n",
    "\n",
    "# Let's filter the dataframe to only include years from 1980 to 2023.\n",
    "j17_historical_data_df = j17_historical_data_df[(j17_historical_data_df['Year'] >= 2000) & (j17_historical_data_df['Year'] <= 2023)].reset_index()\n",
    "\n",
    "# Find the value in the \"dw_date\" column that is in the same row as the max value in the \"date\" column and print the first one out\n",
    "latest_date = j17_historical_data_df[j17_historical_data_df['DailyHighDate'] == j17_historical_data_df['DailyHighDate'].max()]['dw_date'].values[0]\n",
    "\n",
    "# Let's filter the dataframe to only include rows where the dw_date is between 2023-01-01 and 2023-03-02\n",
    "j17_historical_data_df = j17_historical_data_df[(j17_historical_data_df['dw_date'] >= '01/01/2023') & (j17_historical_data_df['dw_date'] <= latest_date)].reset_index()\n",
    "\n",
    "# Pivot the j17_historical_data_df so that each year is a column and dw_date is the index with the values being the \"DailyHigh\" column. This is so that we can easily visualize it in a Datawrapper line chart.\n",
    "j17_historical_data_df = j17_historical_data_df.pivot(index='dw_date', columns='Year', values='WaterLevelElevation').reset_index()\n",
    "\n",
    "# Export j17_historical_data_df to a csv file\n",
    "j17_historical_data_df.to_csv('../output/aquifers/j17_historical_data.csv', index=False)\n",
    "\n",
    "# Let's take a look at the first 5 rows of the dataframe\n",
    "j17_historical_data_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The lakes\n",
    "\n",
    "Now that we have the Edwards Aquifer data, we'll grab the water level data for several nearby lakes. The lakes we'll be using are:\n",
    "- Medina Lake\n",
    "- Canyon Lake\n",
    "- Lake Travis\n",
    "\n",
    "We're sourcing this data from [waterdatafortexas.org](https://waterdatafortexas.org/reservoirs/statewide), a product from [the Texas Water Development Board](https://www.twdb.texas.gov/). Their mission \"is to lead the state's efforts in ensuring a secure water future for Texas and its citizens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>water_level</th>\n",
       "      <th>surface_area</th>\n",
       "      <th>reservoir_storage</th>\n",
       "      <th>conservation_storage</th>\n",
       "      <th>percent_full</th>\n",
       "      <th>conservation_capacity</th>\n",
       "      <th>dead_pool_capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1940-09-30</td>\n",
       "      <td>550.7</td>\n",
       "      <td>1864.0</td>\n",
       "      <td>44232</td>\n",
       "      <td>22807</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1113531</td>\n",
       "      <td>21425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1940-10-31</td>\n",
       "      <td>548.2</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>39722</td>\n",
       "      <td>18297</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1113531</td>\n",
       "      <td>21425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1940-11-30</td>\n",
       "      <td>595.5</td>\n",
       "      <td>5209.0</td>\n",
       "      <td>194535</td>\n",
       "      <td>173110</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1113531</td>\n",
       "      <td>21425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1940-12-31</td>\n",
       "      <td>615.0</td>\n",
       "      <td>7320.0</td>\n",
       "      <td>315950</td>\n",
       "      <td>294525</td>\n",
       "      <td>26.4</td>\n",
       "      <td>1113531</td>\n",
       "      <td>21425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1941-01-31</td>\n",
       "      <td>614.0</td>\n",
       "      <td>7205.0</td>\n",
       "      <td>308688</td>\n",
       "      <td>287263</td>\n",
       "      <td>25.8</td>\n",
       "      <td>1113531</td>\n",
       "      <td>21425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  water_level  surface_area  reservoir_storage  \\\n",
       "0  1940-09-30        550.7        1864.0              44232   \n",
       "1  1940-10-31        548.2        1750.0              39722   \n",
       "2  1940-11-30        595.5        5209.0             194535   \n",
       "3  1940-12-31        615.0        7320.0             315950   \n",
       "4  1941-01-31        614.0        7205.0             308688   \n",
       "\n",
       "   conservation_storage  percent_full  conservation_capacity  \\\n",
       "0                 22807           2.0                1113531   \n",
       "1                 18297           1.6                1113531   \n",
       "2                173110          15.5                1113531   \n",
       "3                294525          26.4                1113531   \n",
       "4                287263          25.8                1113531   \n",
       "\n",
       "   dead_pool_capacity  \n",
       "0               21425  \n",
       "1               21425  \n",
       "2               21425  \n",
       "3               21425  \n",
       "4               21425  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's import the lake data from the waterdatafortexas.org website. We don't need to use BeautifulSoup for this because the data appears to be located at a static URL. We skip rows because there's a bunch of metadata at the top of the CSV file that we don't need.\n",
    "medina_lake_full_data_df = pd.read_csv(\"https://www.waterdatafortexas.org/reservoirs/individual/medina.csv\", skiprows=55)\n",
    "canyon_lake_full_data_df = pd.read_csv(\"https://www.waterdatafortexas.org/reservoirs/individual/canyon.csv\", skiprows=54)\n",
    "travis_lake_full_data_df = pd.read_csv(\"https://www.waterdatafortexas.org/reservoirs/individual/travis.csv\", skiprows=53)\n",
    "\n",
    "# I'll print out the head of medina_lake_full_data_df so that we can see what it looks like ahead of time.\n",
    "travis_lake_full_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>water_level</th>\n",
       "      <th>surface_area</th>\n",
       "      <th>reservoir_storage</th>\n",
       "      <th>conservation_storage</th>\n",
       "      <th>percent_full</th>\n",
       "      <th>conservation_capacity</th>\n",
       "      <th>dead_pool_capacity</th>\n",
       "      <th>Lake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-08-09</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>6662.23</td>\n",
       "      <td>304449</td>\n",
       "      <td>254823</td>\n",
       "      <td>100.0</td>\n",
       "      <td>254823</td>\n",
       "      <td>0</td>\n",
       "      <td>Medina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-08-10</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>6662.23</td>\n",
       "      <td>304449</td>\n",
       "      <td>254823</td>\n",
       "      <td>100.0</td>\n",
       "      <td>254823</td>\n",
       "      <td>0</td>\n",
       "      <td>Medina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-08-11</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>6662.23</td>\n",
       "      <td>304449</td>\n",
       "      <td>254823</td>\n",
       "      <td>100.0</td>\n",
       "      <td>254823</td>\n",
       "      <td>0</td>\n",
       "      <td>Medina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-08-12</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>6662.23</td>\n",
       "      <td>304449</td>\n",
       "      <td>254823</td>\n",
       "      <td>100.0</td>\n",
       "      <td>254823</td>\n",
       "      <td>0</td>\n",
       "      <td>Medina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-08-13</td>\n",
       "      <td>1071.9</td>\n",
       "      <td>6653.70</td>\n",
       "      <td>303783</td>\n",
       "      <td>254823</td>\n",
       "      <td>100.0</td>\n",
       "      <td>254823</td>\n",
       "      <td>0</td>\n",
       "      <td>Medina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  water_level  surface_area  reservoir_storage  \\\n",
       "0  1997-08-09       1072.0       6662.23             304449   \n",
       "1  1997-08-10       1072.0       6662.23             304449   \n",
       "2  1997-08-11       1072.0       6662.23             304449   \n",
       "3  1997-08-12       1072.0       6662.23             304449   \n",
       "4  1997-08-13       1071.9       6653.70             303783   \n",
       "\n",
       "   conservation_storage  percent_full  conservation_capacity  \\\n",
       "0                254823         100.0                 254823   \n",
       "1                254823         100.0                 254823   \n",
       "2                254823         100.0                 254823   \n",
       "3                254823         100.0                 254823   \n",
       "4                254823         100.0                 254823   \n",
       "\n",
       "   dead_pool_capacity    Lake  \n",
       "0                   0  Medina  \n",
       "1                   0  Medina  \n",
       "2                   0  Medina  \n",
       "3                   0  Medina  \n",
       "4                   0  Medina  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to have all the lakes in a single dataframe so that we can easily clean all of the data at once. I'll create a new dataframe called \"lake_data_df\" that contains all of the data from the other dataframes. Before exporting the data to a CSV file, I'll break it up into individual dataframes again.\n",
    "\n",
    "# Before we do that, let's create a new column called \"Lake\" that contains the name of the lake. We'll use this column to filter the data later on.\n",
    "medina_lake_full_data_df['Lake'] = 'Medina'\n",
    "canyon_lake_full_data_df['Lake'] = 'Canyon'\n",
    "travis_lake_full_data_df['Lake'] = 'Travis'\n",
    "\n",
    "# Let's create a new dataframe called \"lake_data_df\" that contains all of the data from the other dataframes.\n",
    "lake_data_df = pd.concat([medina_lake_full_data_df, canyon_lake_full_data_df, travis_lake_full_data_df])\n",
    "\n",
    "# Let's take a look at the first 5 rows of the dataframe\n",
    "lake_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Year</th>\n",
       "      <th>dw_date</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>...</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2050</td>\n",
       "      <td>93.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.2</td>\n",
       "      <td>85.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84.4</td>\n",
       "      <td>75.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>92.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>89.0</td>\n",
       "      <td>99.4</td>\n",
       "      <td>79.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/02/2050</td>\n",
       "      <td>93.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.2</td>\n",
       "      <td>85.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84.4</td>\n",
       "      <td>75.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>92.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>88.9</td>\n",
       "      <td>99.4</td>\n",
       "      <td>79.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/03/2050</td>\n",
       "      <td>93.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.2</td>\n",
       "      <td>85.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84.3</td>\n",
       "      <td>75.6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>92.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>88.9</td>\n",
       "      <td>99.3</td>\n",
       "      <td>79.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/04/2050</td>\n",
       "      <td>93.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.2</td>\n",
       "      <td>85.1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84.3</td>\n",
       "      <td>75.6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>92.6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>88.9</td>\n",
       "      <td>99.2</td>\n",
       "      <td>79.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/05/2050</td>\n",
       "      <td>93.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.7</td>\n",
       "      <td>99.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.2</td>\n",
       "      <td>85.1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84.3</td>\n",
       "      <td>75.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>92.6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.3</td>\n",
       "      <td>88.9</td>\n",
       "      <td>99.2</td>\n",
       "      <td>79.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Year     dw_date  2000   2001   2002   2003  2004   2005  2006  2007   2008  \\\n",
       "0     01/01/2050  93.4  100.0  100.0  100.0  99.7  100.0  95.2  85.0  100.0   \n",
       "1     01/02/2050  93.3  100.0  100.0  100.0  99.7  100.0  95.2  85.0  100.0   \n",
       "2     01/03/2050  93.4  100.0  100.0  100.0  99.7  100.0  95.2  85.0  100.0   \n",
       "3     01/04/2050  93.4  100.0  100.0  100.0  99.8  100.0  95.2  85.1  100.0   \n",
       "4     01/05/2050  93.3  100.0  100.0   99.7  99.8  100.0  95.2  85.1  100.0   \n",
       "\n",
       "Year  ...  2014  2015   2016   2017  2018   2019  2020  2021  2022  2023  \n",
       "0     ...  84.4  75.5  100.0  100.0  92.7  100.0  93.4  89.0  99.4  79.6  \n",
       "1     ...  84.4  75.5  100.0  100.0  92.7  100.0  93.4  88.9  99.4  79.6  \n",
       "2     ...  84.3  75.6  100.0  100.0  92.7  100.0  93.4  88.9  99.3  79.6  \n",
       "3     ...  84.3  75.6  100.0  100.0  92.6  100.0  93.4  88.9  99.2  79.5  \n",
       "4     ...  84.3  75.5  100.0  100.0  92.6  100.0  93.3  88.9  99.2  79.4  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alright, let's do some cleaning.\n",
    "\n",
    "# Let's convert the date column of each dataframe to a datetime object\n",
    "lake_data_df['date'] = pd.to_datetime(lake_data_df['date'])\n",
    "\n",
    "# Let's create a new column titled \"Year\" that contains the year of the \"date\" column\n",
    "# medina_lake_full_data_df['Year'] = medina_lake_full_data_df['date'].dt.year\n",
    "lake_data_df['Year'] = lake_data_df['date'].dt.year\n",
    "\n",
    "# Let's create a new column titled \"dw_date\" that duplicates the data found in the \"date\" column, but replaces the year with 2050. We're doing all of this to make it easier for us to visualize it in a Datawrapper line chart in the future. The 2050 year will not appear in the final product and has no bearing on our analysis.\n",
    "lake_data_df['dw_date'] = lake_data_df['date'].dt.strftime('%m/%d') + '/2050'\n",
    "\n",
    "# Find the value in the \"dw_date\" column that is in the same row as the max value in the \"date\" column and print the first one out\n",
    "latest_date = lake_data_df[lake_data_df['date'] == lake_data_df['date'].max()]['dw_date'].iloc[0]\n",
    "\n",
    "# Let's only keep records where the \"Year\" column in the lake_data_df is greater than or equal to 2000\n",
    "lake_data_df = lake_data_df[lake_data_df['Year'] >= 2000]\n",
    "\n",
    "# Let's filter the dataframe to only include rows where the dw_date is between 2050-01-01 and 2050-03-20\n",
    "lake_data_df = lake_data_df[(lake_data_df['dw_date'] >= '01/01/2050') & (lake_data_df['dw_date'] <= latest_date)].reset_index()\n",
    "\n",
    "# Let's break up the lake_data_df dataframe into individual dataframes again.\n",
    "medina_lake_full_data_df = lake_data_df[lake_data_df['Lake'] == 'Medina']\n",
    "canyon_lake_full_data_df = lake_data_df[lake_data_df['Lake'] == 'Canyon']\n",
    "travis_lake_full_data_df = lake_data_df[lake_data_df['Lake'] == 'Travis']\n",
    "\n",
    "# Pivot the medina_lake_full_data_df so that each year is a column and dw_date is the index with the values being the \"percent_full\" column. This is so that we can easily visualize it in a Datawrapper line chart.\n",
    "medina_lake_full_data_df = medina_lake_full_data_df.pivot(index='dw_date', columns='Year', values='percent_full').reset_index()\n",
    "canyon_lake_full_data_df = canyon_lake_full_data_df.pivot(index='dw_date', columns='Year', values='percent_full').reset_index()\n",
    "travis_lake_full_data_df = travis_lake_full_data_df.pivot(index='dw_date', columns='Year', values='percent_full').reset_index()\n",
    "\n",
    "# Export each of the dataframes to a csv file\n",
    "medina_lake_full_data_df.to_csv('../output/lakes/medina_lake_full_data.csv', index=False)\n",
    "canyon_lake_full_data_df.to_csv('../output/lakes/canyon_lake_full_data.csv', index=False)\n",
    "travis_lake_full_data_df.to_csv('../output/lakes/travis_lake_full_data.csv', index=False)\n",
    "\n",
    "canyon_lake_full_data_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rivers\n",
    "\n",
    "Now that we have the Edwards Aquifer data and the lake data, we'll grab the discharge data for several nearby rivers.\n",
    "\n",
    "The data are coming from the USGS [National Water Information System](https://waterdata.usgs.gov/nwis/rt) (NWIS) through their [dataretrieval-python library](https://github.com/DOI-USGS/dataretrieval-python).\n",
    "\n",
    " The rivers we'll be using are:\n",
    "- Guadalupe River at Spring Branch, Texas\n",
    "- Guadalupe River at Sattler, Texas\n",
    "- Guadalupe River at New Braunfels, Texas\n",
    "- San Marcos River in San Marcos, Texas\n",
    "- Blanco River at Wimberley, Texas\n",
    "- Medina River at Patterson Road in Medina, Texas\n",
    "- Medina River at La Coste, Texas\n",
    "- San Antonio River near Floresvilles, Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>River</th>\n",
       "      <th>site_no</th>\n",
       "      <th>datetime</th>\n",
       "      <th>streamflow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guadalupe Rv nr Spring Branch</td>\n",
       "      <td>08167500</td>\n",
       "      <td>2000-01-01 00:00:00+00:00</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Guadalupe Rv nr Spring Branch</td>\n",
       "      <td>08167500</td>\n",
       "      <td>2000-01-02 00:00:00+00:00</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Guadalupe Rv nr Spring Branch</td>\n",
       "      <td>08167500</td>\n",
       "      <td>2000-01-03 00:00:00+00:00</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guadalupe Rv nr Spring Branch</td>\n",
       "      <td>08167500</td>\n",
       "      <td>2000-01-04 00:00:00+00:00</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guadalupe Rv nr Spring Branch</td>\n",
       "      <td>08167500</td>\n",
       "      <td>2000-01-05 00:00:00+00:00</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           River   site_no                  datetime  \\\n",
       "0  Guadalupe Rv nr Spring Branch  08167500 2000-01-01 00:00:00+00:00   \n",
       "1  Guadalupe Rv nr Spring Branch  08167500 2000-01-02 00:00:00+00:00   \n",
       "2  Guadalupe Rv nr Spring Branch  08167500 2000-01-03 00:00:00+00:00   \n",
       "3  Guadalupe Rv nr Spring Branch  08167500 2000-01-04 00:00:00+00:00   \n",
       "4  Guadalupe Rv nr Spring Branch  08167500 2000-01-05 00:00:00+00:00   \n",
       "\n",
       "   streamflow  \n",
       "0        86.0  \n",
       "1        89.0  \n",
       "2        89.0  \n",
       "3        86.0  \n",
       "4        88.0  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start by creating a dictionary that contains the name of the rivers we're interested in and its site number. We'll use this dictionary to loop through the data and create a dataframe for each river.\n",
    "rivers = {\n",
    "    'Guadalupe Rv nr Spring Branch': '08167500',\n",
    "    'Guadalupe Rv at Sattler': '08167800',\n",
    "    'Guadalupe Rv Abv Comal Rv at New Braunfels': '08168500',\n",
    "    'San Marcos Rv at San Marcos': '08170500',\n",
    "    'Blanco Rv at Wimberley': '08171000',\n",
    "    'Medina Rv at Patterson Rd at Medina': '0817887350',\n",
    "    'Medina Rv at La Coste': '08180640',\n",
    "    'San Antonio Rv nr Floresville': '08183200',\n",
    "}\n",
    "\n",
    "# Convert the values in the dictionary to a list\n",
    "rivers = list(rivers.values())\n",
    "\n",
    "unified_rivers_df = nwis.get_record(sites=rivers, service='dv', start='2000-01-01', parameterCd='00060')\n",
    "\n",
    "# Reset the index so that site_no is a column\n",
    "unified_rivers_df = unified_rivers_df.reset_index()\n",
    "\n",
    "# Create a new column called \"River\" that contains the name of the river based on the site number. Refer to the dictionary above to see which site number corresponds to which river.\n",
    "unified_rivers_df['River'] = unified_rivers_df['site_no'].map({\n",
    "    '08167500': 'Guadalupe Rv nr Spring Branch',\n",
    "    '08167800': 'Guadalupe Rv at Sattler',\n",
    "    '08168500': 'Guadalupe Rv Abv Comal Rv at New Braunfels',\n",
    "    '08170500': 'San Marcos Rv at San Marcos',\n",
    "    '08171000': 'Blanco Rv at Wimberley',\n",
    "    '0817887350': 'Medina Rv at Patterson Rd at Medina',\n",
    "    '08180640': 'Medina Rv at La Coste',\n",
    "    '08183200': 'San Antonio Rv nr Floresville',\n",
    "})\n",
    "\n",
    "unified_rivers_df.to_csv('../output/rivers/unified_rivers.csv', index=False)\n",
    "\n",
    "# Let's reorganize the columns so that the \"River\" column is the first column\n",
    "unified_rivers_df = unified_rivers_df[['River', 'site_no', 'datetime', '00060_Mean']]\n",
    "\n",
    "# Rename 00060_Maximum to \"streamflow\"\n",
    "unified_rivers_df = unified_rivers_df.rename(columns={'00060_Mean': 'streamflow'})\n",
    "\n",
    "unified_rivers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>River</th>\n",
       "      <th>site_no</th>\n",
       "      <th>datetime</th>\n",
       "      <th>streamflow</th>\n",
       "      <th>Year</th>\n",
       "      <th>dw_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guadalupe Rv nr Spring Branch</td>\n",
       "      <td>08167500</td>\n",
       "      <td>2000-01-01 00:00:00+00:00</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>01/01/2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Guadalupe Rv nr Spring Branch</td>\n",
       "      <td>08167500</td>\n",
       "      <td>2000-01-02 00:00:00+00:00</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>01/02/2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Guadalupe Rv nr Spring Branch</td>\n",
       "      <td>08167500</td>\n",
       "      <td>2000-01-03 00:00:00+00:00</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>01/03/2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guadalupe Rv nr Spring Branch</td>\n",
       "      <td>08167500</td>\n",
       "      <td>2000-01-04 00:00:00+00:00</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>01/04/2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guadalupe Rv nr Spring Branch</td>\n",
       "      <td>08167500</td>\n",
       "      <td>2000-01-05 00:00:00+00:00</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>01/05/2050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           River   site_no                  datetime  \\\n",
       "0  Guadalupe Rv nr Spring Branch  08167500 2000-01-01 00:00:00+00:00   \n",
       "1  Guadalupe Rv nr Spring Branch  08167500 2000-01-02 00:00:00+00:00   \n",
       "2  Guadalupe Rv nr Spring Branch  08167500 2000-01-03 00:00:00+00:00   \n",
       "3  Guadalupe Rv nr Spring Branch  08167500 2000-01-04 00:00:00+00:00   \n",
       "4  Guadalupe Rv nr Spring Branch  08167500 2000-01-05 00:00:00+00:00   \n",
       "\n",
       "   streamflow  Year     dw_date  \n",
       "0        86.0  2000  01/01/2050  \n",
       "1        89.0  2000  01/02/2050  \n",
       "2        89.0  2000  01/03/2050  \n",
       "3        86.0  2000  01/04/2050  \n",
       "4        88.0  2000  01/05/2050  "
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the datetime column to a datetime object\n",
    "unified_rivers_df['datetime'] = pd.to_datetime(unified_rivers_df['datetime'])\n",
    "\n",
    "# Create a new column called \"Year\" that contains the year of the datetime column\n",
    "unified_rivers_df['Year'] = unified_rivers_df['datetime'].dt.year\n",
    "\n",
    "# Let's create a new column titled \"dw_date\" that duplicates the data found in the \"date\" column, but replaces the year with 2050. We're doing all of this to make it easier for us to visualize it in a Datawrapper line chart in the future. The 2050 year will not appear in the final product and has no bearing on our analysis.\n",
    "unified_rivers_df['dw_date'] = unified_rivers_df['datetime'].dt.strftime('%m/%d') + '/2050'\n",
    "\n",
    "# Find the value in the \"dw_date\" column that is in the same row as the max value in the \"date\" column and print the first one out\n",
    "latest_date = unified_rivers_df[unified_rivers_df['datetime'] == unified_rivers_df['datetime'].max()]['dw_date'].iloc[0]\n",
    "\n",
    "# Let's filter the dataframe to only contain rows where the Year is greater than or equal to 2000\n",
    "unified_rivers_df = unified_rivers_df[unified_rivers_df['Year'] >= 2000]\n",
    "\n",
    "# Let's filter the dataframe to only include rows where the dw_date is between 2050-01-01 and 2050-03-20\n",
    "unified_rivers_df = unified_rivers_df[(unified_rivers_df['dw_date'] >= '01/01/2050') & (unified_rivers_df['dw_date'] <= latest_date)].reset_index(drop=True)\n",
    "\n",
    "# Break up the dataframe into separate csv files for each River\n",
    "for river in unified_rivers_df['River'].unique():\n",
    "    river_df = unified_rivers_df[unified_rivers_df['River'] == river]\n",
    "\n",
    "    # Pivot the dataframe so that each year is a column and dw_date is the index with the values being the \"streamflow\" column. This is so that we can easily visualize it in a Datawrapper line chart.\n",
    "    river_df = river_df.pivot(index='dw_date', columns='Year', values='streamflow').reset_index()\n",
    "    # If a row has any missing values, drop it\n",
    "    river_df = river_df.dropna()\n",
    "\n",
    "    # Replace spaces with underscores in the file name\n",
    "    river = river.replace(' ', '_')\n",
    "    river_df.to_csv(f'../output/rivers/{river}.csv', index=False)\n",
    "\n",
    "# unified_rivers_df.to_csv('../output/rivers/river_data.csv')\n",
    "unified_rivers_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "This section is for playing around with the data. It's not part of the collection of data, but it's useful for exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/djgt59l94t51k1198f2hnn980000gn/T/ipykernel_37495/4172157322.py:2: FutureWarning: The default value of numeric_only in DataFrame.median is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  j17_historical_data_median_df = j17_historical_data_df.median().to_frame().reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Year</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2005</td>\n",
       "      <td>696.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>693.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2008</td>\n",
       "      <td>688.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>2019</td>\n",
       "      <td>686.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>2017</td>\n",
       "      <td>684.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2002</td>\n",
       "      <td>682.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>2004</td>\n",
       "      <td>680.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>679.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>676.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2010</td>\n",
       "      <td>676.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>2020</td>\n",
       "      <td>672.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2011</td>\n",
       "      <td>671.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>668.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>668.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>2016</td>\n",
       "      <td>666.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "      <td>664.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21</td>\n",
       "      <td>2021</td>\n",
       "      <td>664.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>663.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22</td>\n",
       "      <td>2022</td>\n",
       "      <td>663.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12</td>\n",
       "      <td>2012</td>\n",
       "      <td>657.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13</td>\n",
       "      <td>2013</td>\n",
       "      <td>653.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "      <td>644.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14</td>\n",
       "      <td>2014</td>\n",
       "      <td>643.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2023</td>\n",
       "      <td>636.910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  Year        0\n",
       "0       5  2005  696.680\n",
       "1       3  2003  693.470\n",
       "2       8  2008  688.170\n",
       "3      19  2019  686.340\n",
       "4      17  2017  684.620\n",
       "5       2  2002  682.250\n",
       "6       4  2004  680.700\n",
       "7       1  2001  679.980\n",
       "8       6  2006  676.340\n",
       "9      10  2010  676.040\n",
       "10     20  2020  672.650\n",
       "11     11  2011  671.800\n",
       "12      9  2009  668.890\n",
       "13      7  2007  668.880\n",
       "14     16  2016  666.030\n",
       "15     18  2018  664.610\n",
       "16     21  2021  664.070\n",
       "17      0  2000  663.945\n",
       "18     22  2022  663.030\n",
       "19     12  2012  657.635\n",
       "20     13  2013  653.840\n",
       "21     15  2015  644.370\n",
       "22     14  2014  643.010\n",
       "23     23  2023  636.910"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the median of each column. Put the results in a new dataframe\n",
    "j17_historical_data_median_df = j17_historical_data_df.median().to_frame().reset_index()\n",
    "\n",
    "# Sort the dataframe by the median values\n",
    "j17_historical_data_median_df = j17_historical_data_median_df.sort_values(by=0, ascending=False).reset_index()\n",
    "\n",
    "j17_historical_data_median_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
